{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "train_samples = 10000\n",
    "\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "def expand(i):\n",
    "    res = [0 for _ in range(0, 10)]\n",
    "    res[i] = 1 \n",
    "    return res\n",
    "\n",
    "y = [expand(int(v)) for v in y]\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import src.FFNN as ffnn\n",
    "from src.utils import plot_training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 1.0185, Val_loss: 0.9008\n",
      "Epoch 2/300, Loss: 0.9000, Val_loss: 0.8996\n",
      "Epoch 3/300, Loss: 0.8996, Val_loss: 0.8996\n",
      "Epoch 4/300, Loss: 0.8996, Val_loss: 0.8995\n",
      "Epoch 5/300, Loss: 0.8995, Val_loss: 0.8995\n",
      "Epoch 6/300, Loss: 0.8995, Val_loss: 0.8994\n",
      "Epoch 7/300, Loss: 0.8994, Val_loss: 0.8993\n",
      "Epoch 8/300, Loss: 0.8993, Val_loss: 0.8992\n",
      "Epoch 9/300, Loss: 0.8991, Val_loss: 0.8990\n",
      "Epoch 10/300, Loss: 0.8989, Val_loss: 0.8987\n",
      "Epoch 11/300, Loss: 0.8986, Val_loss: 0.8982\n",
      "Epoch 12/300, Loss: 0.8980, Val_loss: 0.8975\n",
      "Epoch 13/300, Loss: 0.8970, Val_loss: 0.8963\n",
      "Epoch 14/300, Loss: 0.8952, Val_loss: 0.8940\n",
      "Epoch 15/300, Loss: 0.8919, Val_loss: 0.8895\n",
      "Epoch 16/300, Loss: 0.8853, Val_loss: 0.8808\n",
      "Epoch 17/300, Loss: 0.8730, Val_loss: 0.8648\n",
      "Epoch 18/300, Loss: 0.8532, Val_loss: 0.8418\n",
      "Epoch 19/300, Loss: 0.8297, Val_loss: 0.8201\n",
      "Epoch 20/300, Loss: 0.8115, Val_loss: 0.8054\n",
      "Epoch 21/300, Loss: 0.7981, Val_loss: 0.7928\n",
      "Epoch 22/300, Loss: 0.7838, Val_loss: 0.7764\n",
      "Epoch 23/300, Loss: 0.7645, Val_loss: 0.7563\n",
      "Epoch 24/300, Loss: 0.7439, Val_loss: 0.7361\n",
      "Epoch 25/300, Loss: 0.7247, Val_loss: 0.7179\n",
      "Epoch 26/300, Loss: 0.7062, Val_loss: 0.6996\n",
      "Epoch 27/300, Loss: 0.6879, Val_loss: 0.6805\n",
      "Epoch 28/300, Loss: 0.6686, Val_loss: 0.6609\n",
      "Epoch 29/300, Loss: 0.6493, Val_loss: 0.6416\n",
      "Epoch 30/300, Loss: 0.6317, Val_loss: 0.6250\n",
      "Epoch 31/300, Loss: 0.6145, Val_loss: 0.6073\n",
      "Epoch 32/300, Loss: 0.5976, Val_loss: 0.5913\n",
      "Epoch 33/300, Loss: 0.5811, Val_loss: 0.5749\n",
      "Epoch 34/300, Loss: 0.5647, Val_loss: 0.5589\n",
      "Epoch 35/300, Loss: 0.5488, Val_loss: 0.5412\n",
      "Epoch 36/300, Loss: 0.5312, Val_loss: 0.5249\n",
      "Epoch 37/300, Loss: 0.5137, Val_loss: 0.5053\n",
      "Epoch 38/300, Loss: 0.4943, Val_loss: 0.4865\n",
      "Epoch 39/300, Loss: 0.4743, Val_loss: 0.4652\n",
      "Epoch 40/300, Loss: 0.4550, Val_loss: 0.4491\n",
      "Epoch 41/300, Loss: 0.4349, Val_loss: 0.4274\n",
      "Epoch 42/300, Loss: 0.4163, Val_loss: 0.4123\n",
      "Epoch 43/300, Loss: 0.4014, Val_loss: 0.3948\n",
      "Epoch 44/300, Loss: 0.3824, Val_loss: 0.3788\n",
      "Epoch 45/300, Loss: 0.3654, Val_loss: 0.3634\n",
      "Epoch 46/300, Loss: 0.3495, Val_loss: 0.3498\n",
      "Epoch 47/300, Loss: 0.3350, Val_loss: 0.3314\n",
      "Epoch 48/300, Loss: 0.3203, Val_loss: 0.3177\n",
      "Epoch 49/300, Loss: 0.3084, Val_loss: 0.3029\n",
      "Epoch 50/300, Loss: 0.2980, Val_loss: 0.3033\n",
      "Epoch 51/300, Loss: 0.2864, Val_loss: 0.2900\n",
      "Epoch 52/300, Loss: 0.2774, Val_loss: 0.2800\n",
      "Epoch 53/300, Loss: 0.2624, Val_loss: 0.2624\n",
      "Epoch 54/300, Loss: 0.2525, Val_loss: 0.2607\n",
      "Epoch 55/300, Loss: 0.2493, Val_loss: 0.2540\n",
      "Epoch 56/300, Loss: 0.2411, Val_loss: 0.2459\n",
      "Epoch 57/300, Loss: 0.2321, Val_loss: 0.2314\n",
      "Epoch 58/300, Loss: 0.2233, Val_loss: 0.2199\n",
      "Epoch 59/300, Loss: 0.2164, Val_loss: 0.2202\n",
      "Epoch 60/300, Loss: 0.2095, Val_loss: 0.2137\n",
      "Epoch 61/300, Loss: 0.2089, Val_loss: 0.2097\n",
      "Epoch 62/300, Loss: 0.2027, Val_loss: 0.2063\n",
      "Epoch 63/300, Loss: 0.1974, Val_loss: 0.1980\n",
      "Epoch 64/300, Loss: 0.1899, Val_loss: 0.1950\n",
      "Epoch 65/300, Loss: 0.1924, Val_loss: 0.2028\n",
      "Epoch 66/300, Loss: 0.1891, Val_loss: 0.1920\n",
      "Epoch 67/300, Loss: 0.1848, Val_loss: 0.1899\n",
      "Epoch 68/300, Loss: 0.1818, Val_loss: 0.1853\n",
      "Epoch 69/300, Loss: 0.1798, Val_loss: 0.1830\n",
      "Epoch 70/300, Loss: 0.1718, Val_loss: 0.1770\n",
      "Epoch 71/300, Loss: 0.1737, Val_loss: 0.1754\n",
      "Epoch 72/300, Loss: 0.1685, Val_loss: 0.1741\n",
      "Epoch 73/300, Loss: 0.1704, Val_loss: 0.1782\n",
      "Epoch 74/300, Loss: 0.1694, Val_loss: 0.1889\n",
      "Epoch 75/300, Loss: 0.1769, Val_loss: 0.1817\n",
      "Epoch 76/300, Loss: 0.1733, Val_loss: 0.1787\n",
      "Epoch 77/300, Loss: 0.1700, Val_loss: 0.1690\n",
      "Epoch 78/300, Loss: 0.1714, Val_loss: 0.1801\n",
      "Epoch 79/300, Loss: 0.1668, Val_loss: 0.1746\n",
      "Epoch 80/300, Loss: 0.1657, Val_loss: 0.1674\n",
      "Epoch 81/300, Loss: 0.1617, Val_loss: 0.1736\n",
      "Epoch 82/300, Loss: 0.1662, Val_loss: 0.1762\n",
      "Epoch 83/300, Loss: 0.1620, Val_loss: 0.1685\n",
      "Epoch 84/300, Loss: 0.1587, Val_loss: 0.1613\n",
      "Epoch 85/300, Loss: 0.1498, Val_loss: 0.1581\n",
      "Epoch 86/300, Loss: 0.1611, Val_loss: 0.1705\n",
      "Epoch 87/300, Loss: 0.1645, Val_loss: 0.1696\n",
      "Epoch 88/300, Loss: 0.1579, Val_loss: 0.1681\n",
      "Epoch 89/300, Loss: 0.1499, Val_loss: 0.1593\n",
      "Epoch 90/300, Loss: 0.1548, Val_loss: 0.1622\n",
      "Epoch 91/300, Loss: 0.1533, Val_loss: 0.1562\n",
      "Epoch 92/300, Loss: 0.1481, Val_loss: 0.1580\n",
      "Epoch 93/300, Loss: 0.1543, Val_loss: 0.1803\n",
      "Epoch 94/300, Loss: 0.1611, Val_loss: 0.1639\n",
      "Epoch 95/300, Loss: 0.1487, Val_loss: 0.1584\n",
      "Epoch 96/300, Loss: 0.1508, Val_loss: 0.1635\n",
      "Epoch 97/300, Loss: 0.1540, Val_loss: 0.1739\n",
      "Epoch 98/300, Loss: 0.1545, Val_loss: 0.1646\n",
      "Epoch 99/300, Loss: 0.1497, Val_loss: 0.1637\n",
      "Epoch 100/300, Loss: 0.1461, Val_loss: 0.1480\n",
      "Epoch 101/300, Loss: 0.1436, Val_loss: 0.1513\n",
      "Epoch 102/300, Loss: 0.1440, Val_loss: 0.1535\n",
      "Epoch 103/300, Loss: 0.1454, Val_loss: 0.1471\n",
      "Epoch 104/300, Loss: 0.1356, Val_loss: 0.1402\n",
      "Epoch 105/300, Loss: 0.1323, Val_loss: 0.1412\n",
      "Epoch 106/300, Loss: 0.1396, Val_loss: 0.1496\n",
      "Epoch 107/300, Loss: 0.1464, Val_loss: 0.1561\n",
      "Epoch 108/300, Loss: 0.1435, Val_loss: 0.1560\n",
      "Epoch 109/300, Loss: 0.1512, Val_loss: 0.1676\n",
      "Epoch 110/300, Loss: 0.1498, Val_loss: 0.1541\n",
      "Epoch 111/300, Loss: 0.1455, Val_loss: 0.1514\n",
      "Epoch 112/300, Loss: 0.1418, Val_loss: 0.1488\n",
      "Epoch 113/300, Loss: 0.1434, Val_loss: 0.1541\n",
      "Epoch 114/300, Loss: 0.1432, Val_loss: 0.1573\n",
      "Epoch 115/300, Loss: 0.1458, Val_loss: 0.1463\n",
      "Epoch 116/300, Loss: 0.1314, Val_loss: 0.1412\n",
      "Epoch 117/300, Loss: 0.1328, Val_loss: 0.1392\n",
      "Epoch 118/300, Loss: 0.1284, Val_loss: 0.1411\n",
      "Epoch 119/300, Loss: 0.1306, Val_loss: 0.1388\n",
      "Epoch 120/300, Loss: 0.1310, Val_loss: 0.1356\n",
      "Epoch 121/300, Loss: 0.1345, Val_loss: 0.1400\n",
      "Epoch 122/300, Loss: 0.1285, Val_loss: 0.1397\n",
      "Epoch 123/300, Loss: 0.1288, Val_loss: 0.1375\n",
      "Epoch 124/300, Loss: 0.1314, Val_loss: 0.1468\n",
      "Epoch 125/300, Loss: 0.1361, Val_loss: 0.1513\n",
      "Epoch 126/300, Loss: 0.1381, Val_loss: 0.1437\n",
      "Epoch 127/300, Loss: 0.1344, Val_loss: 0.1519\n",
      "Epoch 128/300, Loss: 0.1308, Val_loss: 0.1506\n",
      "Epoch 129/300, Loss: 0.1322, Val_loss: 0.1407\n",
      "Epoch 130/300, Loss: 0.1294, Val_loss: 0.1383\n",
      "Epoch 131/300, Loss: 0.1348, Val_loss: 0.1407\n",
      "Epoch 132/300, Loss: 0.1260, Val_loss: 0.1405\n",
      "Epoch 133/300, Loss: 0.1291, Val_loss: 0.1381\n",
      "Epoch 134/300, Loss: 0.1264, Val_loss: 0.1401\n",
      "Epoch 135/300, Loss: 0.1268, Val_loss: 0.1338\n",
      "Epoch 136/300, Loss: 0.1242, Val_loss: 0.1360\n",
      "Epoch 137/300, Loss: 0.1292, Val_loss: 0.1359\n",
      "Epoch 138/300, Loss: 0.1307, Val_loss: 0.1351\n",
      "Epoch 139/300, Loss: 0.1277, Val_loss: 0.1359\n",
      "Epoch 140/300, Loss: 0.1289, Val_loss: 0.1360\n",
      "Epoch 141/300, Loss: 0.1316, Val_loss: 0.1385\n",
      "Epoch 142/300, Loss: 0.1242, Val_loss: 0.1306\n",
      "Epoch 143/300, Loss: 0.1187, Val_loss: 0.1362\n",
      "Epoch 144/300, Loss: 0.1235, Val_loss: 0.1288\n",
      "Epoch 145/300, Loss: 0.1160, Val_loss: 0.1294\n",
      "Epoch 146/300, Loss: 0.1210, Val_loss: 0.1384\n",
      "Epoch 147/300, Loss: 0.1273, Val_loss: 0.1441\n",
      "Epoch 148/300, Loss: 0.1227, Val_loss: 0.1310\n",
      "Epoch 149/300, Loss: 0.1216, Val_loss: 0.1372\n",
      "Epoch 150/300, Loss: 0.1222, Val_loss: 0.1315\n",
      "Epoch 151/300, Loss: 0.1181, Val_loss: 0.1280\n",
      "Epoch 152/300, Loss: 0.1131, Val_loss: 0.1210\n",
      "Epoch 153/300, Loss: 0.1133, Val_loss: 0.1308\n",
      "Epoch 154/300, Loss: 0.1287, Val_loss: 0.1434\n",
      "Epoch 155/300, Loss: 0.1298, Val_loss: 0.1420\n",
      "Epoch 156/300, Loss: 0.1231, Val_loss: 0.1331\n",
      "Epoch 157/300, Loss: 0.1205, Val_loss: 0.1256\n",
      "Epoch 158/300, Loss: 0.1181, Val_loss: 0.1324\n",
      "Epoch 159/300, Loss: 0.1144, Val_loss: 0.1239\n",
      "Epoch 160/300, Loss: 0.1198, Val_loss: 0.1263\n",
      "Epoch 161/300, Loss: 0.1172, Val_loss: 0.1288\n",
      "Epoch 162/300, Loss: 0.1196, Val_loss: 0.1248\n",
      "Epoch 163/300, Loss: 0.1221, Val_loss: 0.1424\n",
      "Epoch 164/300, Loss: 0.1253, Val_loss: 0.1295\n",
      "Epoch 165/300, Loss: 0.1229, Val_loss: 0.1356\n",
      "Epoch 166/300, Loss: 0.1244, Val_loss: 0.1321\n",
      "Epoch 167/300, Loss: 0.1265, Val_loss: 0.1343\n",
      "Epoch 168/300, Loss: 0.1235, Val_loss: 0.1338\n",
      "Epoch 169/300, Loss: 0.1230, Val_loss: 0.1302\n",
      "Epoch 170/300, Loss: 0.1197, Val_loss: 0.1280\n",
      "Epoch 171/300, Loss: 0.1170, Val_loss: 0.1262\n",
      "Epoch 172/300, Loss: 0.1192, Val_loss: 0.1366\n",
      "Epoch 173/300, Loss: 0.1210, Val_loss: 0.1269\n",
      "Epoch 174/300, Loss: 0.1254, Val_loss: 0.1321\n",
      "Epoch 175/300, Loss: 0.1231, Val_loss: 0.1344\n",
      "Epoch 176/300, Loss: 0.1225, Val_loss: 0.1384\n",
      "Epoch 177/300, Loss: 0.1216, Val_loss: 0.1289\n",
      "Epoch 178/300, Loss: 0.1162, Val_loss: 0.1276\n",
      "Epoch 179/300, Loss: 0.1179, Val_loss: 0.1304\n",
      "Epoch 180/300, Loss: 0.1162, Val_loss: 0.1262\n",
      "Epoch 181/300, Loss: 0.1180, Val_loss: 0.1234\n",
      "Epoch 182/300, Loss: 0.1173, Val_loss: 0.1207\n",
      "Epoch 183/300, Loss: 0.1127, Val_loss: 0.1249\n",
      "Epoch 184/300, Loss: 0.1126, Val_loss: 0.1269\n",
      "Epoch 185/300, Loss: 0.1160, Val_loss: 0.1278\n",
      "Epoch 186/300, Loss: 0.1165, Val_loss: 0.1259\n",
      "Epoch 187/300, Loss: 0.1181, Val_loss: 0.1300\n",
      "Epoch 188/300, Loss: 0.1142, Val_loss: 0.1294\n",
      "Epoch 189/300, Loss: 0.1170, Val_loss: 0.1296\n",
      "Epoch 190/300, Loss: 0.1165, Val_loss: 0.1242\n",
      "Epoch 191/300, Loss: 0.1150, Val_loss: 0.1263\n",
      "Epoch 192/300, Loss: 0.1108, Val_loss: 0.1199\n",
      "Epoch 193/300, Loss: 0.1081, Val_loss: 0.1280\n",
      "Epoch 194/300, Loss: 0.1079, Val_loss: 0.1185\n",
      "Epoch 195/300, Loss: 0.1072, Val_loss: 0.1157\n",
      "Epoch 196/300, Loss: 0.1096, Val_loss: 0.1209\n",
      "Epoch 197/300, Loss: 0.1118, Val_loss: 0.1170\n",
      "Epoch 198/300, Loss: 0.1087, Val_loss: 0.1161\n",
      "Epoch 199/300, Loss: 0.1075, Val_loss: 0.1182\n",
      "Epoch 200/300, Loss: 0.1059, Val_loss: 0.1162\n",
      "Epoch 201/300, Loss: 0.1047, Val_loss: 0.1144\n",
      "Epoch 202/300, Loss: 0.1048, Val_loss: 0.1182\n",
      "Epoch 203/300, Loss: 0.1084, Val_loss: 0.1157\n",
      "Epoch 204/300, Loss: 0.1073, Val_loss: 0.1231\n",
      "Epoch 205/300, Loss: 0.1068, Val_loss: 0.1153\n",
      "Epoch 206/300, Loss: 0.1018, Val_loss: 0.1116\n",
      "Epoch 207/300, Loss: 0.1009, Val_loss: 0.1160\n",
      "Epoch 208/300, Loss: 0.1043, Val_loss: 0.1144\n",
      "Epoch 209/300, Loss: 0.1064, Val_loss: 0.1130\n",
      "Epoch 210/300, Loss: 0.1020, Val_loss: 0.1162\n",
      "Epoch 211/300, Loss: 0.1037, Val_loss: 0.1163\n",
      "Epoch 212/300, Loss: 0.1056, Val_loss: 0.1126\n",
      "Epoch 213/300, Loss: 0.1027, Val_loss: 0.1135\n",
      "Epoch 214/300, Loss: 0.1041, Val_loss: 0.1141\n",
      "Epoch 215/300, Loss: 0.1092, Val_loss: 0.1179\n",
      "Epoch 216/300, Loss: 0.1106, Val_loss: 0.1216\n",
      "Epoch 217/300, Loss: 0.1113, Val_loss: 0.1247\n",
      "Epoch 218/300, Loss: 0.1178, Val_loss: 0.1287\n",
      "Epoch 219/300, Loss: 0.1212, Val_loss: 0.1361\n",
      "Epoch 220/300, Loss: 0.1171, Val_loss: 0.1175\n",
      "Epoch 221/300, Loss: 0.1134, Val_loss: 0.1285\n",
      "Epoch 222/300, Loss: 0.1137, Val_loss: 0.1222\n",
      "Epoch 223/300, Loss: 0.1127, Val_loss: 0.1180\n",
      "Epoch 224/300, Loss: 0.1096, Val_loss: 0.1187\n",
      "Epoch 225/300, Loss: 0.1122, Val_loss: 0.1126\n",
      "Epoch 226/300, Loss: 0.1077, Val_loss: 0.1122\n",
      "Epoch 227/300, Loss: 0.1060, Val_loss: 0.1085\n",
      "Epoch 228/300, Loss: 0.1015, Val_loss: 0.1089\n",
      "Epoch 229/300, Loss: 0.1097, Val_loss: 0.1147\n",
      "Epoch 230/300, Loss: 0.1053, Val_loss: 0.1165\n",
      "Epoch 231/300, Loss: 0.1058, Val_loss: 0.1144\n",
      "Epoch 232/300, Loss: 0.1040, Val_loss: 0.1143\n",
      "Epoch 233/300, Loss: 0.1016, Val_loss: 0.1090\n",
      "Epoch 234/300, Loss: 0.0986, Val_loss: 0.1180\n",
      "Epoch 235/300, Loss: 0.1031, Val_loss: 0.1165\n",
      "Epoch 236/300, Loss: 0.1035, Val_loss: 0.1128\n",
      "Epoch 237/300, Loss: 0.1038, Val_loss: 0.1127\n",
      "Epoch 238/300, Loss: 0.1030, Val_loss: 0.1137\n",
      "Epoch 239/300, Loss: 0.1023, Val_loss: 0.1163\n",
      "Epoch 240/300, Loss: 0.1029, Val_loss: 0.1126\n",
      "Epoch 241/300, Loss: 0.1020, Val_loss: 0.1100\n",
      "Epoch 242/300, Loss: 0.0996, Val_loss: 0.1114\n",
      "Epoch 243/300, Loss: 0.1012, Val_loss: 0.1101\n",
      "Epoch 244/300, Loss: 0.1003, Val_loss: 0.1068\n",
      "Epoch 245/300, Loss: 0.1009, Val_loss: 0.1096\n",
      "Epoch 246/300, Loss: 0.0997, Val_loss: 0.1079\n",
      "Epoch 247/300, Loss: 0.0992, Val_loss: 0.1127\n",
      "Epoch 248/300, Loss: 0.1012, Val_loss: 0.1172\n",
      "Epoch 249/300, Loss: 0.1051, Val_loss: 0.1144\n",
      "Epoch 250/300, Loss: 0.1029, Val_loss: 0.1166\n",
      "Epoch 251/300, Loss: 0.1047, Val_loss: 0.1175\n",
      "Epoch 252/300, Loss: 0.1045, Val_loss: 0.1145\n",
      "Epoch 253/300, Loss: 0.1011, Val_loss: 0.1125\n",
      "Epoch 254/300, Loss: 0.1001, Val_loss: 0.1124\n",
      "Epoch 255/300, Loss: 0.0986, Val_loss: 0.1091\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m activations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m sigmoid \u001b[38;5;241m=\u001b[39m ffnn\u001b[38;5;241m.\u001b[39mFFNN(layer_sizes\u001b[38;5;241m=\u001b[39mlayer_size, activations\u001b[38;5;241m=\u001b[39mactivations, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight_init_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m73\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m plot_sigmoid \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Kuliah\\Tingkat 3\\6\\ML\\tubes1\\ML_TUBES_1\\src\\FFNN.py:116\u001b[0m, in \u001b[0;36mFFNN.fit\u001b[1;34m(self, X_train, Y_train, epochs, learning_rate, batch_size, verbose, X_val, Y_val)\u001b[0m\n\u001b[0;32m    113\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m indices[start_idx:end_idx]\n\u001b[0;32m    114\u001b[0m X_batch, Y_batch \u001b[38;5;241m=\u001b[39m X_train[batch_indices], Y_train[batch_indices]  \u001b[38;5;66;03m# Use batch_indices\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m n_samples\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_weights(learning_rate)\n",
      "File \u001b[1;32md:\\Kuliah\\Tingkat 3\\6\\ML\\tubes1\\ML_TUBES_1\\src\\FFNN.py:85\u001b[0m, in \u001b[0;36mFFNN.backward\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Compute weight gradients and bias gradients\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 85\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_value\n",
      "File \u001b[1;32md:\\Kuliah\\Tingkat 3\\6\\ML\\tubes1\\ML_TUBES_1\\src\\Layer.py:33\u001b[0m, in \u001b[0;36mLayer.update_gradient\u001b[1;34m(self, delta, ancestor)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, delta: np\u001b[38;5;241m.\u001b[39mndarray, ancestor: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mancestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     delta \u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation\u001b[38;5;241m.\u001b[39mdo_ds(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_combinations)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m.\u001b[39mT, delta)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "layer_size = [784, 128, 128, 64, 10]\n",
    "activations = [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"]\n",
    "\n",
    "sigmoid = ffnn.FFNN(layer_sizes=layer_size, activations=activations, loss=\"mse\", weight_initializer=\"normal\", weight_init_args={\"seed\": 73})\n",
    "plot_sigmoid = sigmoid.fit(X_train, y_train, 100, 0.1, 50, True, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sigmoid.predict(X_test)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average=\"weighted\")\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid.plot_gradient_weight()\n",
    "sigmoid.plot_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_size = [784, 128, 128, 64, 10]\n",
    "activations = [\"relu\" for i in range(4)]\n",
    "\n",
    "relu = ffnn.FFNN(layer_sizes=layer_size, activations=activations, loss=\"mse\", weight_initializer=\"normal\", weight_init_args={\"seed\": 73})\n",
    "plot_relu = relu.fit(X_train, y_train, 300, 0.1, 50, True, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = relu.predict(X_test)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average=\"weighted\")\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu.plot_gradient_weight()\n",
    "relu.plot_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_size = [784, 128, 128, 64, 10]\n",
    "activations = [\"linear\" for i in range(4)]\n",
    "\n",
    "linear = ffnn.FFNN(layer_sizes=layer_size, activations=activations, loss=\"mse\", weight_initializer=\"normal\", weight_init_args={\"seed\": 73})\n",
    "plot_linear = linear.fit(X_train, y_train, 300, 0.1, 50, True, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear.predict(X_test)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average=\"weighted\")\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.plot_gradient_weight()\n",
    "linear.plot_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_size = [784, 128, 128, 64, 10]\n",
    "activations = [\"tanh\" for i in range(4)]\n",
    "\n",
    "tanh = ffnn.FFNN(layer_sizes=layer_size, activations=activations, loss=\"mse\", weight_initializer=\"normal\", weight_init_args={\"seed\": 73})\n",
    "plot_tanh = tanh.fit(X_train, y_train, 300, 0.1, 50, True, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tanh.predict(X_test)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average=\"weighted\")\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh.plot_gradient_weight()\n",
    "tanh.plot_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_size = [784, 128, 128, 64, 10]\n",
    "activations = [\"selu\" for i in range(4)]\n",
    "\n",
    "selu = ffnn.FFNN(layer_sizes=layer_size, activations=activations, loss=\"mse\", weight_initializer=\"normal\", weight_init_args={\"seed\": 73})\n",
    "plot_selu = selu.fit(X_train, y_train, 300, 0.1, 50, True, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = selu.predict(X_test)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average=\"weighted\")\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selu.plot_gradient_weight()\n",
    "selu.plot_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_size = [784, 128, 128, 64, 10]\n",
    "activations = [\"leaky\" for i in range(4)]\n",
    "\n",
    "leaky = ffnn.FFNN(layer_sizes=layer_size, activations=activations, loss=\"mse\", weight_initializer=\"normal\", weight_init_args={\"seed\": 73})\n",
    "plot_leaky = leaky.fit(X_train, y_train, 300, 0.1, 50, True, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = leaky.predict(X_test)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average=\"weighted\")\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky.plot_gradient_weight()\n",
    "leaky.plot_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results([plot_sigmoid,plot_relu,plot_linear,plot_tanh,plot_selu,plot_leaky])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
